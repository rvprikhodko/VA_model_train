{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca761cd8-8bd3-4119-81f7-385c39d3c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import pandas as pd\n",
    "from utils.preprocess import denoise_with_sitk, denoise_with_ants, enhance_vessels_with_frangi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ef96880-9898-4d2a-9b0b-a535c6a88974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vertebral_artery_dataset(pairs):\n",
    "    \n",
    "    data = []\n",
    "    for img_path, mask_path in pairs:\n",
    "        if not os.path.exists(img_path) or not os.path.exists(mask_path):\n",
    "            print(f\"Пропускаю отсутствующие файлы: {img_path} или {mask_path}\")\n",
    "            continue\n",
    "        \n",
    "        data.append({\n",
    "            'image_path': img_path,\n",
    "            'mask_path': mask_path\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "data_dir = './dataset'\n",
    "\n",
    "image_files = glob.glob(os.path.join(data_dir, '*.jpg'))\n",
    "mask_files = [f.replace('.jpg', '-VA.png') for f in image_files]\n",
    "pairs = [(img, mask) for img, mask in zip(image_files, mask_files)]\n",
    "\n",
    "dataset = create_vertebral_artery_dataset(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707ac241-3ef5-4e76-aeb6-b98faff01e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_split_dataset(\n",
    "    dataset,\n",
    "    output_base_dir,\n",
    "    method='sitk',  # 'sitk', 'ants', 'frangi'\n",
    "    val_ratio=0.2,\n",
    "    random_state=42\n",
    "):\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    \n",
    "    # Разделяем на train и val\n",
    "    train_data, val_data = train_test_split(dataset, test_size=val_ratio, random_state=random_state)\n",
    "    \n",
    "    splits = {'train': train_data, 'val': val_data}\n",
    "    \n",
    "    for split_name, split_data in splits.items():\n",
    "        split_dir = os.path.join(output_base_dir, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        for idx, row in tqdm(split_data.iterrows(), total=len(split_data), desc=f\"Processing {split_name} data\"):\n",
    "            try:\n",
    "                image_path = row['image_path']\n",
    "                mask_path = row['mask_path']\n",
    "                \n",
    "                image_filename = os.path.basename(image_path)\n",
    "                mask_filename = os.path.basename(mask_path)\n",
    "                \n",
    "                name_img, ext_img = os.path.splitext(image_filename)\n",
    "                name_mask, ext_mask = os.path.splitext(mask_filename)\n",
    "                \n",
    "                new_img_filename = f\"{name_img}{ext_img}\"\n",
    "                new_mask_filename = f\"{name_mask}{ext_mask}\"\n",
    "                \n",
    "                output_img_path = os.path.join(split_dir, new_img_filename)\n",
    "                output_mask_path = os.path.join(split_dir, new_mask_filename)\n",
    "                \n",
    "                # Проверяем, существуют ли уже обработанные файлы\n",
    "                if os.path.exists(output_img_path) and os.path.exists(output_mask_path):\n",
    "                    continue\n",
    "                \n",
    "                # Загружаем изображение\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image_array = np.array(image)\n",
    "                \n",
    "                # Обрабатываем изображение в зависимости от метода\n",
    "                if method == 'sitk':\n",
    "                    denoised_image = np.zeros_like(image_array)\n",
    "                    for i in range(3):\n",
    "                        denoised_image[:, :, i] = denoise_with_sitk(image_array[:, :, i])\n",
    "                    processed_image = Image.fromarray(denoised_image.astype('uint8'))\n",
    "                    \n",
    "                elif method == 'ants':\n",
    "                    grayscale_image = image.convert('L')\n",
    "                    grayscale_array = np.array(grayscale_image)\n",
    "                    denoised_array = denoise_with_ants(grayscale_array)\n",
    "                    rgb_denoised = np.stack([denoised_array]*3, axis=2)\n",
    "                    processed_image = Image.fromarray(rgb_denoised.astype('uint8'))\n",
    "                    \n",
    "                elif method == 'frangi':\n",
    "                    grayscale_image = image.convert('L')\n",
    "                    grayscale_array = np.array(grayscale_image)\n",
    "                    enhanced_vessels = enhance_vessels_with_frangi(grayscale_array)\n",
    "                    enhanced_rgb = np.stack([enhanced_vessels]*3, axis=-1)\n",
    "                    processed_image = Image.fromarray(enhanced_rgb.astype('uint8'))\n",
    "                    \n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown method: {method}\")\n",
    "                \n",
    "                # Сохраняем обработанное изображение\n",
    "                processed_image.save(output_img_path)\n",
    "                \n",
    "                # Копируем маску без изменений\n",
    "                if os.path.exists(mask_path):\n",
    "                    shutil.copy(mask_path, output_mask_path)\n",
    "                else:\n",
    "                    print(f\"Warning: Mask file not found: {mask_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing files: Image: {image_path}, Mask: {mask_path}\")\n",
    "                print(f\"Error details: {str(e)}\")\n",
    "    \n",
    "    print(f\"Processing complete. Dataset saved to {output_base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75d99def-44df-499a-8975-11154cc11ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 11999.44it/s]\n",
      "Processing val data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 5999.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Dataset saved to new_dataset/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_and_split_dataset(dataset, 'new_dataset/', method='frangi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd5089-fcc7-42f5-9a60-20d7fef77564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
